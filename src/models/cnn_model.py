import numpy as np
from layers.conv_layer import Conv2DLayer
from layers.pooling_layer import MaxPool2DLayer, GlobalAveragePool2DLayer
from layers.dense_layer import DenseLayer
from layers.activation_layer import ActivationLayer
from layers.flatten_layer import FlattenLayer

class CNNModel:
    """
    Modelo de Red Neuronal Convolucional optimizado para clasificaci√≥n de im√°genes.
    
    Esta clase implementa una CNN completa que puede ser construida de manera modular
    a√±adiendo capas secuencialmente. Incluye funcionalidades de compilaci√≥n autom√°tica,
    propagaci√≥n hacia adelante y hacia atr√°s, y predicci√≥n.
    
    La arquitectura se construye siguiendo el patr√≥n:
    Input -> Conv+Activation+Pooling -> ... -> Flatten/GAP -> Dense+Activation -> Output
    
    Attributes:
        layers (list): Lista de capas que componen el modelo
        compiled (bool): Indica si el modelo ha sido compilado (par√°metros inicializados)
    """
    
    def __init__(self):
        """Inicializa un modelo CNN vac√≠o."""
        self.layers = []
        self.compiled = False
        
    def add_layer(self, layer):
        """
        A√±ade una capa al modelo.
        
        Las capas se procesan secuencialmente en el orden que se a√±aden.
        
        Args:
            layer: Instancia de cualquier capa que herede de BaseLayer
        """
        self.layers.append(layer)
        
    def compile(self, input_shape):
        """
        Compila el modelo inicializando los par√°metros de todas las capas.
        
        Este proceso es crucial porque:
        1. Calcula las formas de salida de cada capa secuencialmente
        2. Inicializa los par√°metros (pesos y sesgos) de capas entrenables
        3. Verifica compatibilidad dimensional entre capas consecutivas
        
        Args:
            input_shape (tuple): Forma de entrada al modelo (channels, height, width)
        """
        current_shape = input_shape
        
        for i, layer in enumerate(self.layers):
            # Inicializar par√°metros si la capa los requiere
            if hasattr(layer, 'initialize_params'):
                # Para capas densas, necesitamos el tama√±o aplanado
                if isinstance(layer, DenseLayer):
                    if len(current_shape) > 1:
                        # Aplanar autom√°ticamente si viene de capas conv
                        input_size = np.prod(current_shape)
                    else:
                        input_size = current_shape[0]
                    
                    layer.initialize_params(int(input_size))
                else:
                    # Para capas conv/pool, usar la forma actual
                    layer.initialize_params(current_shape)
            
            # Calcular forma de salida para la siguiente capa
            if hasattr(layer, 'get_output_shape'):
                current_shape = layer.get_output_shape(current_shape)
            else:
                # Fallback para capas sin get_output_shape definido
                current_shape = self._calculate_output_shape(layer, current_shape)
        
        self.compiled = True
        print(f"Modelo compilado exitosamente con {len(self.layers)} capas")
        
    def _calculate_output_shape(self, layer, input_shape):
        """
        Calcula la forma de salida para capas que no implementan get_output_shape.
        
        Args:
            layer: Capa para la cual calcular la forma de salida
            input_shape: Forma de entrada a la capa
            
        Returns:
            tuple: Forma de salida de la capa
        """
        if isinstance(layer, Conv2DLayer):
            channels, height, width = input_shape
            out_height = (height + 2 * layer.padding - layer.filter_size) // layer.stride + 1
            out_width = (width + 2 * layer.padding - layer.filter_size) // layer.stride + 1
            return (layer.num_filters, out_height, out_width)
            
        elif isinstance(layer, MaxPool2DLayer):
            channels, height, width = input_shape
            out_height = (height - layer.pool_size) // layer.stride + 1
            out_width = (width - layer.pool_size) // layer.stride + 1
            return (channels, out_height, out_width)
            
        elif isinstance(layer, GlobalAveragePool2DLayer):
            channels, height, width = input_shape
            return (channels,)
            
        elif isinstance(layer, FlattenLayer):
            return (np.prod(input_shape),)
            
        elif isinstance(layer, DenseLayer):
            return (layer.units,)
            
        else:
            # Para capas de activaci√≥n y otras, mantener la misma forma
            return input_shape
        
    def forward(self, inputs):
        """
        Propagaci√≥n hacia adelante a trav√©s de todas las capas.
        
        Procesa la entrada secuencialmente a trav√©s de cada capa,
        donde la salida de una capa se convierte en la entrada de la siguiente.
        
        Args:
            inputs (np.ndarray): Tensor de entrada al modelo
                                Forma: (batch_size, channels, height, width)
                                
        Returns:
            np.ndarray: Salida del modelo (logits o probabilidades)
                       Forma: (batch_size, num_classes)
        """
        if not self.compiled:
            raise RuntimeError("El modelo debe ser compilado antes de usarse. Llame a model.compile(input_shape)")
        
        x = inputs
        for layer in self.layers:
            x = layer.forward(x)
        return x
        
    def backward(self, grad_output):
        """
        Propagaci√≥n hacia atr√°s a trav√©s de todas las capas.
        
        Calcula gradientes propag√°ndolos desde la salida hacia la entrada,
        permitiendo que cada capa calcule sus gradientes de par√°metros.
        
        Args:
            grad_output (np.ndarray): Gradientes de la funci√≥n de p√©rdida
                                     
        Returns:
            np.ndarray: Gradientes respecto a la entrada del modelo
        """
        grad = grad_output
        # Procesar capas en orden inverso
        for layer in reversed(self.layers):
            grad = layer.backward(grad)
        return grad
        
    def predict(self, inputs):
        """
        Realiza predicciones devolviendo las clases predichas.
        
        Args:
            inputs (np.ndarray): Datos de entrada para predicci√≥n
                                
        Returns:
            np.ndarray: Clases predichas (√≠ndices de clase)
                       Forma: (batch_size,)
        """
        outputs = self.forward(inputs)
        return np.argmax(outputs, axis=1)
        
    def predict_proba(self, inputs):
        """
        Realiza predicciones devolviendo probabilidades de clase.
        
        Aplica softmax a la salida para obtener una distribuci√≥n de probabilidad
        sobre las clases, incluso si el modelo no tiene softmax en la √∫ltima capa.
        
        Args:
            inputs (np.ndarray): Datos de entrada para predicci√≥n
                                
        Returns:
            np.ndarray: Probabilidades de clase
                       Forma: (batch_size, num_classes)
        """
        outputs = self.forward(inputs)
        
        # Aplicar softmax para obtener probabilidades
        # Estabilidad num√©rica: restar el m√°ximo
        exp_outputs = np.exp(outputs - np.max(outputs, axis=1, keepdims=True))
        probabilities = exp_outputs / np.sum(exp_outputs, axis=1, keepdims=True)
        
        return probabilities
        
    def get_params(self):
        """
        Obtiene todos los par√°metros del modelo.
        
        √ötil para guardar el modelo entrenado o para an√°lisis de par√°metros.
        
        Returns:
            dict: Diccionario con los par√°metros de todas las capas
        """
        params = {}
        for i, layer in enumerate(self.layers):
            if hasattr(layer, 'params') and layer.params:
                params[f'layer_{i}'] = layer.get_params()
        return params
        
    def set_params(self, params):
        """
        Establece los par√°metros del modelo.
        
        √ötil para cargar un modelo previamente entrenado.
        
        Args:
            params (dict): Diccionario con par√°metros de las capas
        """
        for i, layer in enumerate(self.layers):
            layer_key = f'layer_{i}'
            if layer_key in params and hasattr(layer, 'params'):
                layer.params = params[layer_key]
        
    def summary(self):
        """
        Imprime un resumen de la arquitectura del modelo.
        
        Muestra informaci√≥n detallada sobre cada capa, incluyendo
        formas de salida y n√∫mero de par√°metros.
        """
        print("=" * 60)
        print("RESUMEN DEL MODELO CNN")
        print("=" * 60)
        
        total_params = 0
        
        for i, layer in enumerate(self.layers):
            layer_name = layer.__class__.__name__
            
            # Contar par√°metros
            layer_params = 0
            if hasattr(layer, 'params'):
                for param_name, param_value in layer.params.items():
                    if param_value is not None:
                        layer_params += np.prod(param_value.shape)
            
            total_params += layer_params
            
            # Informaci√≥n adicional seg√∫n el tipo de capa
            extra_info = ""
            if isinstance(layer, Conv2DLayer):
                extra_info = f"({layer.num_filters} filtros, {layer.filter_size}x{layer.filter_size})"
            elif isinstance(layer, MaxPool2DLayer):
                extra_info = f"({layer.pool_size}x{layer.pool_size})"
            elif isinstance(layer, DenseLayer):
                extra_info = f"({layer.units} unidades)"
            elif isinstance(layer, ActivationLayer):
                extra_info = f"({layer.activation})"
            
            print(f"Capa {i+1:2d}: {layer_name:<20} {extra_info:<20} Par√°metros: {layer_params:,}")
        
        print("=" * 60)
        print(f"TOTAL DE PAR√ÅMETROS: {total_params:,}")
        print("=" * 60)


def create_retina_cnn(input_shape, num_classes=5):
    """
    Crea un modelo CNN optimizado para clasificaci√≥n de retinopat√≠a diab√©tica.
    
    Esta arquitectura est√° dise√±ada espec√≠ficamente para im√°genes m√©dicas de retina,
    con una estructura que balancea capacidad de extracci√≥n de caracter√≠sticas
    con eficiencia computacional.
    
    Arquitectura:
    - 2 bloques convolucionales ligeros (16, 32 filtros)
    - Max pooling para reducci√≥n dimensional
    - Flatten para transici√≥n a capas densas
    - Capa densa intermedia (128 unidades)
    - Capa de salida con softmax
    
    Esta arquitectura reducida es m√°s eficiente que versiones m√°s profundas
    mientras mantiene capacidad de aprendizaje adecuada.
    
    Args:
        input_shape (tuple): Forma de entrada (channels, height, width)
        num_classes (int, optional): N√∫mero de clases a clasificar. Default: 5
        
    Returns:
        CNNModel: Modelo CNN compilado y listo para entrenar
    """
    model = CNNModel()
    
    # ====== BLOQUE CONVOLUCIONAL 1 ======
    # Filtros: 16, Tama√±o: 3x3, Padding: 1 (mantiene dimensiones)
    model.add_layer(Conv2DLayer(num_filters=16, filter_size=3, padding=1))
    model.add_layer(ActivationLayer('relu'))  # Introducir no-linealidad
    model.add_layer(MaxPool2DLayer(pool_size=2, stride=2))  # Reducir dimensiones 2x
    
    # ====== BLOQUE CONVOLUCIONAL 2 ======
    # Filtros: 32, Tama√±o: 3x3, Padding: 1
    model.add_layer(Conv2DLayer(num_filters=32, filter_size=3, padding=1))
    model.add_layer(ActivationLayer('relu'))
    model.add_layer(MaxPool2DLayer(pool_size=2, stride=2))  # Reducir dimensiones 2x
    
    # ====== TRANSICI√ìN A CAPAS DENSAS ======
    # Aplanar caracter√≠sticas extra√≠das por capas convolucionales
    model.add_layer(FlattenLayer())
    
    # ====== CAPAS DENSAS ======
    # Capa intermedia para aprendizaje de patrones complejos
    model.add_layer(DenseLayer(units=128))
    model.add_layer(ActivationLayer('relu'))
    
    # Capa de salida para clasificaci√≥n
    model.add_layer(DenseLayer(units=num_classes))
    model.add_layer(ActivationLayer('softmax'))  # Probabilidades de clase
    
    # ====== COMPILACI√ìN ======
    # Inicializar par√°metros y verificar compatibilidad dimensional
    model.compile(input_shape)
    
    print("üß† Modelo CNN para Retinopat√≠a Diab√©tica creado exitosamente")
    print(f"   üìä Entrada: {input_shape}")
    print(f"   üéØ Clases: {num_classes}")
    print(f"   ‚ö° Arquitectura optimizada para velocidad y precisi√≥n")
    
    return model